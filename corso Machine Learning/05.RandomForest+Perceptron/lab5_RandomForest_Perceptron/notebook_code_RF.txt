import numpy as np
import matplotlib.pyplot as plt
plt.ion()

cmap = 'jet'


def plot_2d_dataset(X, Y, title=''):
    """
    Plots a two-dimensional dataset.

    Parameters
    ----------
    X: ndarray
        data points. (shape:(n_samples, dim))
    Y: ndarray
        groundtruth labels. (shape:(n_samples,))
    title: str
        an optional title for the plot.
    """

    # new figure
    plt.figure()

    # set lims
    x_min = np.min(X[:, 0])
    x_max = np.max(X[:, 0])
    y_min = np.min(X[:, 1])
    y_max = np.max(X[:, 1])
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)

    # remove ticks
    plt.xticks(())
    plt.yticks(())

    # plot points
    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, s=40, cmap=cmap, edgecolors='k')
    plt.title(title)
    plt.show()
    #plt.waitforbuttonpress()


def plot_boundary(X, Y, model, title=''):
    """
    Represents the boundaries of a generic learning model over data.

    Parameters
    ----------
    X: ndarray
        data points. (shape:(n_samples, dim))
    Y: ndarray
        groundtruth labels. (shape:(n_samples,))
    model: SVC
        A sklearn.SVC fit model.
    title: str
        an optional title for the plot.
    """

    # initialize subplots
    fig, ax = plt.subplots(1, 2)
    ax[0].scatter(X[:, 0], X[:, 1], c=Y, s=40, zorder=10, cmap=cmap, edgecolors='k')

    # evaluate lims
    x_min = np.min(X[:, 0])
    x_max = np.max(X[:, 0])
    y_min = np.min(X[:, 1])
    y_max = np.max(X[:, 1])

    # predict all over a grid
    XX, YY = np.mgrid[x_min:x_max:500j, y_min:y_max:500j]
    Z = model.predict(np.c_[XX.ravel(), YY.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(XX.shape)
    ax[1].pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)
    ax[1].scatter(X[:, 0], X[:, 1], c=Y, s=40, zorder=10, cmap=cmap, edgecolors='k')

    # set stuff for subplots
    for s in [0, 1]:
        ax[s].set_xlim([x_min, x_max])
        ax[s].set_ylim([y_min, y_max])
        ax[s].set_xticks([])
        ax[s].set_yticks([])

    ax[0].set_title('Data')
    ax[1].set_title('Boundary')

    plt.show()
    #plt.waitforbuttonpress()

import numpy as np
from sklearn.datasets import make_moons

def gaussians_dataset(n_gaussian, n_points, mus, stds):
    """
    Provides a dataset made by several gaussians.

    Parameters
    ----------
    n_gaussian : int
        The number of desired gaussian components.
    n_points : list
        A list of cardinality of points (one for each gaussian).
    mus : list
        A list of means (one for each gaussian, e.g. [[1, 1], [3, 1]).
    stds : list
        A list of stds (one for each gaussian, e.g. [[1, 1], [2, 2]).

    Returns
    -------
    tuple
        a tuple like:
            X_train ndarray shape: (n_samples, dims).
            Y_train ndarray shape: (n_samples,).
            X_test ndarray shape: (n_samples, dims).
            Y_test ndarray shape: (n_samples,).
    """

    assert n_gaussian == len(mus) == len(stds) == len(n_points)

    X = []
    Y = []
    for i in range(0, n_gaussian):

        mu = mus[i]
        std = stds[i]
        n_pt = n_points[i]

        cov = np.diag(std)

        X.append(np.random.multivariate_normal(mu, cov, size=2*n_pt))
        Y.append(np.ones(shape=2*n_pt) * i)

    X = np.concatenate(X, axis=0)
    Y = np.concatenate(Y, axis=0)

    tot = np.concatenate((X, np.reshape(Y, newshape=(-1, 1))), axis=-1)

    np.random.seed(30101990)
    np.random.shuffle(tot)
    X = tot[:, :-1]
    Y = tot[:, -1]

    n_train_samples = X.shape[0]//2

    X_train = X[:n_train_samples]
    Y_train = Y[:n_train_samples]

    X_test = X[n_train_samples:]
    Y_test = Y[n_train_samples:]

    Y_train[Y_train == 0] = -1
    Y_test[Y_test == 0] = -1

    return X_train, Y_train, X_test, Y_test


def two_moon_dataset(n_samples=100, shuffle=True, noise=None, random_state=None):
    """
    Make two interleaving half circles

    A simple toy dataset to visualize clustering and classification
    algorithms.

    Parameters
    ----------
    n_samples : int, optional (default=100)
        The total number of points generated.

    shuffle : bool, optional (default=True)
        Whether to shuffle the samples.

    noise : double or None (default=None)
        Standard deviation of Gaussian noise added to the data.

    Read more in the :ref:`User Guide <sample_generators>`.

    Returns
    -------
    tuple
        a tuple like:
            X_train ndarray shape: (n_samples, dims).
            Y_train ndarray shape: (n_samples,).
            X_test ndarray shape: (n_samples, dims).
            Y_test ndarray shape: (n_samples,).
    """
    X_train, Y_train = make_moons(n_samples, shuffle, noise, random_state)
    X_test, Y_test = make_moons(n_samples, shuffle, noise, random_state)

    Y_train[Y_train == 0] = -1
    Y_test[Y_test == 0] = -1

    return X_train, Y_train, X_test, Y_test


def h_shaped_dataset():
    """
    Yet another dataset to experiment with boosting.
    It returns a complex non-linear binary dataset.

    Returns
    -------
    tuple
        a tuple like:
            X_train ndarray shape: (n_samples, dims).
            Y_train ndarray shape: (n_samples,).
            X_test ndarray shape: (n_samples, dims).
            Y_test ndarray shape: (n_samples,).
    """

    data = np.load('data/data.npy')
    labels = np.squeeze(np.load('data/labels.npy'))
    # shuffle
    n, d  = data.shape
    idx = np.arange(0, n)
    np.random.shuffle(idx)

    X_train = data[idx[:n//2]]
    Y_train = labels[idx[:n // 2]]

    X_test = data[idx[n//2:]]
    Y_test = labels[idx[n//2:]]

    return X_train, Y_train, X_test, Y_test

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

plt.ion()

def main_RF():
    """
    Main function for testing Adaboost.
    """

    X_train, Y_train, X_test, Y_test = h_shaped_dataset()
    #X_train, Y_train, X_test, Y_test = gaussians_dataset(2, [100, 150], [[1, 3], [-4, 8]], [[2, 3], [4, 1]])
    #X_train, Y_train, X_test, Y_test = two_moon_dataset(n_samples=300, noise=0.2)

    # visualize dataset
    plot_2d_dataset(X_train, Y_train, 'Training')

    classifier = RandomForestClassifier(n_estimators=100, max_depth=10)
    classifier.fit(X_train, Y_train)
    P = classifier.predict(X_test)

    # visualize the boundary!
    plot_boundary(X_train, Y_train, classifier)

    # evaluate and print error
    error = float(np.sum(P != Y_test)) / Y_test.size
    print('Classification error: {}'.format(error))


# entry point
if __name__ == '__main__':
    main_RF()
